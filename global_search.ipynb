{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global search\n",
    "\n",
    "ベースラインRAGは、答えを構成するためにデータセット全体の情報の集約を必要とするクエリに苦戦している。ベースラインRAGは、データセット内の意味的に類似したテキストコンテンツのベクトル検索に依存しているため、「データ内のトップ5のテーマは何か」といったクエリのパフォーマンスは最悪である。クエリには、正しい情報へ誘導するものは何もない。\n",
    "\n",
    "しかし、GraphRAGを使えば、LLMが生成した知識グラフの構造から、データセット全体の構造（ひいてはテーマ）を知ることができるため、このような質問に答えることができる。これにより、プライベートデータセットを、あらかじめ要約された意味のあるセマンティッククラスタに整理することができる。LLMは、我々の大域的検索法を用いて、ユーザからのクエリに応答する際に、これらのクラスターを使ってこれらのテーマを要約する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "\n",
    "from graphrag.query.indexer_adapters import read_indexer_entities, read_indexer_reports\n",
    "from graphrag.query.llm.oai.chat_openai import ChatOpenAI\n",
    "from graphrag.query.llm.oai.typing import OpenaiApiType\n",
    "from graphrag.query.structured_search.global_search.community_context import (\n",
    "    GlobalCommunityContext,\n",
    ")\n",
    "from graphrag.query.structured_search.global_search.search import GlobalSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ベースとなるLLMの指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024 Microsoft Corporation.\n",
    "# Licensed under the MIT License\n",
    "\n",
    "\"\"\"Chat-based OpenAI LLM implementation.\"\"\"\n",
    "\n",
    "from collections.abc import Callable\n",
    "from typing import Any\n",
    "\n",
    "from tenacity import (\n",
    "    AsyncRetrying,\n",
    "    RetryError,\n",
    "    Retrying,\n",
    "    retry_if_exception_type,\n",
    "    stop_after_attempt,\n",
    "    wait_exponential_jitter,\n",
    ")\n",
    "\n",
    "from graphrag.query.llm.base import BaseLLM, BaseLLMCallback\n",
    "from graphrag.query.llm.oai.base import OpenAILLMImpl\n",
    "from graphrag.query.llm.oai.typing import (\n",
    "    OPENAI_RETRY_ERROR_TYPES,\n",
    "    OpenaiApiType,\n",
    ")\n",
    "from openai import OpenAI\n",
    "from graphrag.query.progress import StatusReporter\n",
    "\n",
    "_MODEL_REQUIRED_MSG = \"model is required\"\n",
    "\n",
    "\n",
    "class ChatAssistant(BaseLLM):\n",
    "    \"\"\"Wrapper for OpenAI ChatCompletion models.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key: str | None = None,\n",
    "        model: str | None = None,\n",
    "        azure_ad_token_provider: Callable | None = None,\n",
    "        deployment_name: str | None = None,\n",
    "        api_base: str | None = None,\n",
    "        api_version: str | None = None,\n",
    "        api_type: OpenaiApiType = OpenaiApiType.OpenAI,\n",
    "        organization: str | None = None,\n",
    "        max_retries: int = 10,\n",
    "        request_timeout: float = 180.0,\n",
    "        retry_error_types: tuple[type[BaseException]] = OPENAI_RETRY_ERROR_TYPES,  # type: ignore\n",
    "        reporter: StatusReporter | None = None,\n",
    "    ):\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        self.model = model\n",
    "        self.retry_error_types = retry_error_types\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        messages: str | list[Any],\n",
    "        streaming: bool = True,\n",
    "        callbacks: list[BaseLLMCallback] | None = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"Generate text.\"\"\"\n",
    "        try:\n",
    "            retryer = Retrying(\n",
    "                stop=stop_after_attempt(self.max_retries),\n",
    "                wait=wait_exponential_jitter(max=10),\n",
    "                reraise=True,\n",
    "                retry=retry_if_exception_type(self.retry_error_types),\n",
    "            )\n",
    "            for attempt in retryer:\n",
    "                with attempt:\n",
    "                    return self._generate(\n",
    "                        messages=messages,\n",
    "                        streaming=streaming,\n",
    "                        callbacks=callbacks,\n",
    "                        **kwargs,\n",
    "                    )\n",
    "        except RetryError as e:\n",
    "            self._reporter.error(\n",
    "                message=\"Error at generate()\", details={self.__class__.__name__: str(e)}\n",
    "            )\n",
    "            return \"\"\n",
    "        else:\n",
    "            # TODO: why not just throw in this case?\n",
    "            return \"\"\n",
    "\n",
    "    async def agenerate(\n",
    "        self,\n",
    "        messages: str | list[Any],\n",
    "        streaming: bool = True,\n",
    "        callbacks: list[BaseLLMCallback] | None = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"Generate text asynchronously.\"\"\"\n",
    "        try:\n",
    "            retryer = AsyncRetrying(\n",
    "                stop=stop_after_attempt(self.max_retries),\n",
    "                wait=wait_exponential_jitter(max=10),\n",
    "                reraise=True,\n",
    "                retry=retry_if_exception_type(self.retry_error_types),  # type: ignore\n",
    "            )\n",
    "            async for attempt in retryer:\n",
    "                with attempt:\n",
    "                    return await self._agenerate(\n",
    "                        messages=messages,\n",
    "                        streaming=streaming,\n",
    "                        callbacks=callbacks,\n",
    "                        **kwargs,\n",
    "                    )\n",
    "        except RetryError as e:\n",
    "            self._reporter.error(f\"Error at agenerate(): {e}\")\n",
    "            return \"\"\n",
    "        else:\n",
    "            # TODO: why not just throw in this case?\n",
    "            return \"\"\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: str | list[Any],\n",
    "        streaming: bool = True,\n",
    "        callbacks: list[BaseLLMCallback] | None = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        model = self.model\n",
    "        if not model:\n",
    "            raise ValueError(_MODEL_REQUIRED_MSG)\n",
    "        response = self.sync_client.chat.completions.create(  # type: ignore\n",
    "            model=model,\n",
    "            messages=messages,  # type: ignore\n",
    "            stream=streaming,\n",
    "            **kwargs,\n",
    "        )  # type: ignore\n",
    "        if streaming:\n",
    "            full_response = \"\"\n",
    "            while True:\n",
    "                try:\n",
    "                    chunk = response.__next__()  # type: ignore\n",
    "                    if not chunk or not chunk.choices:\n",
    "                        continue\n",
    "\n",
    "                    delta = (\n",
    "                        chunk.choices[0].delta.content\n",
    "                        if chunk.choices[0].delta and chunk.choices[0].delta.content\n",
    "                        else \"\"\n",
    "                    )  # type: ignore\n",
    "\n",
    "                    full_response += delta\n",
    "                    if callbacks:\n",
    "                        for callback in callbacks:\n",
    "                            callback.on_llm_new_token(delta)\n",
    "                    if chunk.choices[0].finish_reason == \"stop\":  # type: ignore\n",
    "                        break\n",
    "                except StopIteration:\n",
    "                    break\n",
    "            return full_response\n",
    "        return response.choices[0].message.content or \"\"  # type: ignore\n",
    "\n",
    "    async def _agenerate(\n",
    "        self,\n",
    "        messages: str | list[Any],\n",
    "        streaming: bool = True,\n",
    "        callbacks: list[BaseLLMCallback] | None = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        model = self.model\n",
    "        if not model:\n",
    "            raise ValueError(_MODEL_REQUIRED_MSG)\n",
    "        response = await self.async_client.chat.completions.create(  # type: ignore\n",
    "            model=model,\n",
    "            messages=messages,  # type: ignore\n",
    "            stream=streaming,\n",
    "            **kwargs,\n",
    "        )\n",
    "        if streaming:\n",
    "            full_response = \"\"\n",
    "            while True:\n",
    "                try:\n",
    "                    chunk = await response.__anext__()  # type: ignore\n",
    "                    if not chunk or not chunk.choices:\n",
    "                        continue\n",
    "\n",
    "                    delta = (\n",
    "                        chunk.choices[0].delta.content\n",
    "                        if chunk.choices[0].delta and chunk.choices[0].delta.content\n",
    "                        else \"\"\n",
    "                    )  # type: ignore\n",
    "\n",
    "                    full_response += delta\n",
    "                    if callbacks:\n",
    "                        for callback in callbacks:\n",
    "                            callback.on_llm_new_token(delta)\n",
    "                    if chunk.choices[0].finish_reason == \"stop\":  # type: ignore\n",
    "                        break\n",
    "                except StopIteration:\n",
    "                    break\n",
    "            return full_response\n",
    "\n",
    "        return response.choices[0].message.content or \"\"  # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "llm_model = \"gpt-4o\"\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    api_key=api_key,\n",
    "    model=llm_model,\n",
    "    api_type=OpenaiApiType.OpenAI,  # OpenaiApiType.OpenAI or OpenaiApiType.AzureOpenAI\n",
    "    max_retries=20,\n",
    ")\n",
    "\n",
    "token_encoder = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "日本の首都は東京です。\n"
     ]
    }
   ],
   "source": [
    "system_message=\"あたなはAIアシスタントです\"\n",
    "user_message=\"日本の首都は？\"\n",
    "print(llm.generate(\n",
    "    messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`python -m graphrag.index --root ./ragtest`で生成された`.parquet`ファイルへのpathを指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir=\"ragtest/output/20240801-212257/artifacts\"\n",
    "\n",
    "community_report_table = \"create_final_community_reports\"\n",
    "entity_table = \"create_final_nodes\"\n",
    "entity_embedding_table = \"create_final_entities\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pd.DataFrame`として読み出し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df = pd.read_parquet(f\"{input_dir}/{community_report_table}.parquet\")\n",
    "entity_df = pd.read_parquet(f\"{input_dir}/{entity_table}.parquet\")\n",
    "entity_embedding_df = pd.read_parquet(f\"{input_dir}/{entity_embedding_table}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrameからGraphRAGとして使える形に変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports = read_indexer_reports(report_df, entity_df, 2)\n",
    "entities = read_indexer_entities(entity_df, entity_embedding_df, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "グローバルな文脈を構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_builder = GlobalCommunityContext(\n",
    "    community_reports=reports,\n",
    "    entities=entities,  # default to None if you don't want to use community weights for ranking\n",
    "    token_encoder=token_encoder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Encoding' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcontext_builder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Encoding' object is not callable"
     ]
    }
   ],
   "source": [
    "context_builder.token_encoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "グローバルサーチのためのエンジンをインスタンス化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_builder_params = {\n",
    "    \"use_community_summary\": False,  # False means using full community reports. True means using community short summaries.\n",
    "    \"shuffle_data\": True,\n",
    "    \"include_community_rank\": True,\n",
    "    \"min_community_rank\": 0,\n",
    "    \"community_rank_name\": \"rank\",\n",
    "    \"include_community_weight\": True,\n",
    "    \"community_weight_name\": \"occurrence weight\",\n",
    "    \"normalize_community_weight\": True,\n",
    "    \"max_tokens\": 12_000,  # change this based on the token limit you have on your model (if you are using a model with 8k limit, a good setting could be 5000)\n",
    "    \"context_name\": \"Reports\",\n",
    "}\n",
    "\n",
    "map_llm_params = {\n",
    "    \"max_tokens\": 1000,\n",
    "    \"temperature\": 0.0,\n",
    "    \"response_format\": {\"type\": \"json_object\"},\n",
    "}\n",
    "\n",
    "reduce_llm_params = {\n",
    "    \"max_tokens\": 2000,  # change this based on the token limit you have on your model (if you are using a model with 8k limit, a good setting could be 1000-1500)\n",
    "    \"temperature\": 0.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_engine = GlobalSearch(\n",
    "    llm=llm,\n",
    "    context_builder=context_builder,\n",
    "    token_encoder=token_encoder,\n",
    "    max_data_tokens=12_000,  # change this based on the token limit you have on your model (if you are using a model with 8k limit, a good setting could be 5000)\n",
    "    map_llm_params=map_llm_params,\n",
    "    reduce_llm_params=reduce_llm_params,\n",
    "    allow_general_knowledge=False,  # set this to True will add instruction to encourage the LLM to incorporate general knowledge in the response, which may increase hallucinations, but could be useful in some use cases.\n",
    "    json_mode=True,  # set this to False if your LLM model does not support JSON mode.\n",
    "    context_builder_params=context_builder_params,\n",
    "    concurrent_coroutines=32,\n",
    "    response_type=\"multiple paragraphs\",  # free form text describing the response type and format, can be anything, e.g. prioritized list, single paragraph, multiple paragraphs, multiple-page report\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "グローバルサーチ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### ドラゴンの存在とその役割\n",
      "\n",
      "ドラゴンは村の中心的な存在であり、ムラのコミュニティにおいて権威とレジリエンスの象徴です。歴史的に村と深い関係を持ち、その存在は村のアイデンティティと物語を形成しています。村人たちはドラゴンの怒りに直面することがあり、そのため恐怖と尊敬が入り混じった複雑なダイナミックが存在します [Data: Reports (3)]。\n",
      "\n",
      "### 村への脅威と対策\n",
      "\n",
      "ドラゴンは村にとって重大な脅威でもあり、村全体がその怒りを和らげる方法を見つけることで安全を確保しなければなりません [Data: Reports (1)]。この緊張感は村の生活に大きな影響を与えています。\n",
      "\n",
      "### 伝統とフォークロアの伝承\n",
      "\n",
      "ドラゴンの存在はムラのフォークロアや伝統の伝承においても重要な役割を果たしています。ドラゴンはコミュニティのレジリエンスを思い起こさせ、村人たちのアイデンティティや自然との関係を形作る要素となっています [Data: Reports (3)]。\n",
      "\n",
      "### コミュニケーションの手段\n",
      "\n",
      "村人の一人であるリナは、石を使ってドラゴンとコミュニケーションを取る能力を持っています。この能力は、コミュニティとドラゴンの間の理解と協力を促進するために不可欠です [Data: Reports (2)]。\n",
      "\n",
      "### ドラゴンの出現場所\n",
      "\n",
      "ドラゴンは湖の近くに現れることが多く、この場所はコミュニティとドラゴンの関係に複雑さを加えています [Data: Reports (1)]。\n",
      "\n",
      "以上の情報から、ドラゴンはムラの生活と文化に深く根付いた存在であり、その影響は多岐にわたります。\n"
     ]
    }
   ],
   "source": [
    "result = await search_engine.asearch(\n",
    "    \"ドラゴンは出てきますか？\"\n",
    ")\n",
    "\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Mura村とドラゴンの関係\n",
      "\n",
      "Mura村は、ドラゴンとの深い歴史的関係を持ち、その関係が村のアイデンティティと物語を形成しています。村人たちはドラゴンの怒りからの脅威に直面し、恐怖と尊敬の複雑なダイナミクスを持っています。この関係は、村の生存と文化的アイデンティティにとって重要です [Data: Reports (3)]。ドラゴンはMura村において恐怖の源であると同時に、権威と村人の闘争を象徴する崇拝の対象でもあります。ドラゴンの存在は、村の物語において重要な文化的遺物として機能し、村人のアイデンティティと自然との関係を形作ります [Data: Reports (3)]。\n",
      "\n",
      "### 村の文化保存とリーダーシップ\n",
      "\n",
      "リオとリナは、Mura村の文化保存において重要な役割を果たしています。リオは村人を保護し、歴史を伝える役割を担い、リナは村人間の調和と平和を促進します。彼らの努力は、村の伝統の継続性と回復力に貢献しています [Data: Reports (3)]。特にリナは、石を使ってドラゴンとコミュニケーションを取る能力を持ち、村人とドラゴンの間の理解と協力を促進する重要な役割を果たしています。彼女の行動は、村とドラゴンの間の潜在的な紛争を解決するために重要です [Data: Reports (2, 1)]。\n",
      "\n",
      "### 自然回復と持続可能性\n",
      "\n",
      "Mura村の自然回復努力は、過去のドラゴンとの紛争による破壊に対する応答であり、村人の持続可能性へのコミットメントを反映しています。この努力は、村人の回復力と未来の世代のための環境保護への献身を強調しています [Data: Reports (3)]。\n",
      "\n",
      "### 結論\n",
      "\n",
      "Mura村は、ドラゴンとの複雑な関係を通じて独自の文化とアイデンティティを築いてきました。リオとリナのリーダーシップは、村の文化保存と調和を促進し、持続可能な未来を目指す村人たちの努力を支えています。これらの要素が組み合わさることで、Mura村はその独自性と回復力を維持し続けています。\n"
     ]
    }
   ],
   "source": [
    "result = await search_engine.asearch(\n",
    "    \"もっと詳しく教えて\"\n",
    ")\n",
    "\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>occurrence weight</th>\n",
       "      <th>content</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Mura and the Dragon's Legacy</td>\n",
       "      <td>1.0</td>\n",
       "      <td># Mura and the Dragon's Legacy\\n\\nThe communit...</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Community of Mura: Rina and Rio</td>\n",
       "      <td>1.0</td>\n",
       "      <td># Community of Mura: Rina and Rio\\n\\nThe commu...</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Dragon's Sand and Eld</td>\n",
       "      <td>0.4</td>\n",
       "      <td># Dragon's Sand and Eld\\n\\nThe community cente...</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Dragon and Lake Community</td>\n",
       "      <td>0.2</td>\n",
       "      <td># Dragon and Lake Community\\n\\nThe community c...</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                            title  occurrence weight  \\\n",
       "0  3     Mura and the Dragon's Legacy                1.0   \n",
       "1  2  Community of Mura: Rina and Rio                1.0   \n",
       "2  0            Dragon's Sand and Eld                0.4   \n",
       "3  1        Dragon and Lake Community                0.2   \n",
       "\n",
       "                                             content  rank  \n",
       "0  # Mura and the Dragon's Legacy\\n\\nThe communit...   8.5  \n",
       "1  # Community of Mura: Rina and Rio\\n\\nThe commu...   7.5  \n",
       "2  # Dragon's Sand and Eld\\n\\nThe community cente...   6.5  \n",
       "3  # Dragon and Lake Community\\n\\nThe community c...   8.0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.context_data[\"reports\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM calls: 2. LLM tokens: 4601\n"
     ]
    }
   ],
   "source": [
    "print(f\"LLM calls: {result.llm_calls}. LLM tokens: {result.prompt_tokens}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
