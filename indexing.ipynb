{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024 Microsoft Corporation.\n",
    "# Licensed under the MIT License\n",
    "\n",
    "\"\"\"Main definition.\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import warnings\n",
    "import uvloop \n",
    "from graphrag.config import create_graphrag_config\n",
    "import os\n",
    "from graphrag.index import PipelineConfig, create_pipeline_config\n",
    "import yaml\n",
    "from graphrag.index.run import run_pipeline_with_config\n",
    "\n",
    "# Ignore warnings from numba\n",
    "warnings.filterwarnings(\"ignore\", message=\".*NumbaDeprecationWarning.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"FutureWarning\")\n",
    "\n",
    "api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "llm_model = \"gpt-4o-mini\"\n",
    "\n",
    "def index(\n",
    "    root: str,\n",
    "    config: str | None,\n",
    "):\n",
    "    \"\"\"Run the pipeline with the given config.\"\"\"\n",
    "    run_id = \"test\"\n",
    "    pipeline_config: str | PipelineConfig = _create_default_config(\n",
    "        root, config\n",
    "    )\n",
    "\n",
    "    def _run_workflow_async() -> None:\n",
    "        async def execute():\n",
    "            async for output in run_pipeline_with_config(\n",
    "                pipeline_config,\n",
    "                run_id=run_id,\n",
    "            ):\n",
    "                if output.errors and len(output.errors) > 0:\n",
    "                    return\n",
    "        \n",
    "        uvloop.install()\n",
    "\n",
    "        # This part replaces asyncio.run(execute())\n",
    "        if not asyncio.get_event_loop().is_running():\n",
    "            asyncio.run(execute())\n",
    "        else:\n",
    "            asyncio.create_task(execute())\n",
    "    _run_workflow_async()\n",
    "\n",
    "def _create_default_config(\n",
    "    root_dir: str,\n",
    "    config_path: str ,\n",
    ") -> PipelineConfig:\n",
    "    \"\"\"Overlay default values on an existing config or create a default config if none is provided.\"\"\"\n",
    "    with open(config_path, \"rb\") as file:\n",
    "        file_content = file.read().decode(encoding=\"utf-8\", errors=\"strict\")\n",
    "        replaced_content = file_content.replace(\"${OPENAI_API_KEY}\", api_key).replace(\"${LLM_MODEL}\", llm_model)\n",
    "        data = yaml.safe_load(replaced_content)\n",
    "        parameters = create_graphrag_config(data, root_dir)\n",
    "\n",
    "    result = create_pipeline_config(parameters)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './data'\n",
    "config = './config/graphrag_index.yaml'\n",
    "\n",
    "# index_cli 関数を呼び出す\n",
    "index(\n",
    "    root=root,\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Copyright (c) 2024 Microsoft Corporation.\n",
    "# Licensed under the MIT License\n",
    "\n",
    "\"\"\"Command line interface for the query module.\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import cast\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from graphrag.config import (\n",
    "    GraphRagConfig,\n",
    "    create_graphrag_config,\n",
    ")\n",
    "from graphrag.index.progress import PrintProgressReporter\n",
    "from graphrag.model.entity import Entity\n",
    "from graphrag.query.input.loaders.dfs import (\n",
    "    store_entity_semantic_embeddings,\n",
    ")\n",
    "from graphrag.vector_stores import VectorStoreFactory, VectorStoreType\n",
    "from graphrag.vector_stores.lancedb import LanceDBVectorStore\n",
    "\n",
    "from graphrag.query.factories import get_global_search_engine, get_local_search_engine\n",
    "from graphrag.query.indexer_adapters import (\n",
    "    read_indexer_covariates,\n",
    "    read_indexer_entities,\n",
    "    read_indexer_relationships,\n",
    "    read_indexer_reports,\n",
    "    read_indexer_text_units,\n",
    ")\n",
    "\n",
    "reporter = PrintProgressReporter(\"\")\n",
    "\n",
    "api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "llm_model = \"gpt-4o-mini\"\n",
    "def __get_embedding_description_store(\n",
    "    entities: list[Entity],\n",
    "    vector_store_type: str = VectorStoreType.LanceDB,\n",
    "    config_args: dict | None = None,\n",
    "):\n",
    "    \"\"\"Get the embedding description store.\"\"\"\n",
    "    if not config_args:\n",
    "        config_args = {}\n",
    "\n",
    "    collection_name = config_args.get(\n",
    "        \"query_collection_name\", \"entity_description_embeddings\"\n",
    "    )\n",
    "    config_args.update({\"collection_name\": collection_name})\n",
    "    description_embedding_store = VectorStoreFactory.get_vector_store(\n",
    "        vector_store_type=vector_store_type, kwargs=config_args\n",
    "    )\n",
    "\n",
    "    description_embedding_store.connect(**config_args)\n",
    "\n",
    "    if config_args.get(\"overwrite\", False):\n",
    "        # this step assumps the embeddings where originally stored in a file rather\n",
    "        # than a vector database\n",
    "\n",
    "        # dump embeddings from the entities list to the description_embedding_store\n",
    "        store_entity_semantic_embeddings(\n",
    "            entities=entities, vectorstore=description_embedding_store\n",
    "        )\n",
    "    else:\n",
    "        # load description embeddings to an in-memory lancedb vectorstore\n",
    "        # to connect to a remote db, specify url and port values.\n",
    "        description_embedding_store = LanceDBVectorStore(\n",
    "            collection_name=collection_name\n",
    "        )\n",
    "        description_embedding_store.connect(\n",
    "            db_uri=config_args.get(\"db_uri\", \"./lancedb\")\n",
    "        )\n",
    "\n",
    "        # load data from an existing table\n",
    "        description_embedding_store.document_collection = (\n",
    "            description_embedding_store.db_connection.open_table(\n",
    "                description_embedding_store.collection_name\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return description_embedding_store\n",
    "\n",
    "\n",
    "def run_global_search(\n",
    "    config_path: str | None,\n",
    "    data_dir: str | None,\n",
    "    root_dir: str | None,\n",
    "    community_level: int,\n",
    "    response_type: str,\n",
    "):\n",
    "    \"\"\"Run a global search with the given query.\"\"\"\n",
    "    config =  _create_default_config(\n",
    "        root_dir, config_path, api_key, llm_model\n",
    "    )\n",
    "    data_path = Path(data_dir)\n",
    "\n",
    "    final_nodes: pd.DataFrame = pd.read_parquet(\n",
    "        data_path / \"create_final_nodes.parquet\"\n",
    "    )\n",
    "    final_entities: pd.DataFrame = pd.read_parquet(\n",
    "        data_path / \"create_final_entities.parquet\"\n",
    "    )\n",
    "    final_community_reports: pd.DataFrame = pd.read_parquet(\n",
    "        data_path / \"create_final_community_reports.parquet\"\n",
    "    )\n",
    "\n",
    "    reports = read_indexer_reports(\n",
    "        final_community_reports, final_nodes, community_level\n",
    "    )\n",
    "    entities = read_indexer_entities(final_nodes, final_entities, community_level)\n",
    "    search_engine = get_global_search_engine(\n",
    "        config,\n",
    "        reports=reports,\n",
    "        entities=entities,\n",
    "        response_type=response_type,\n",
    "    )\n",
    "\n",
    "    return search_engine\n",
    "\n",
    "\n",
    "def run_local_search(\n",
    "    config_path: str | None,\n",
    "    data_dir: str | None,\n",
    "    root_dir: str | None,\n",
    "    community_level: int,\n",
    "    response_type: str,\n",
    "    query: str,\n",
    "):\n",
    "    \"\"\"Run a local search with the given query.\"\"\"\n",
    "    config =  _create_default_config(\n",
    "        root_dir ,config_path, api_key, llm_model\n",
    "    )\n",
    "    data_path = Path(data_dir)\n",
    "\n",
    "    final_nodes = pd.read_parquet(data_path / \"create_final_nodes.parquet\")\n",
    "    final_community_reports = pd.read_parquet(\n",
    "        data_path / \"create_final_community_reports.parquet\"\n",
    "    )\n",
    "    final_text_units = pd.read_parquet(data_path / \"create_final_text_units.parquet\")\n",
    "    final_relationships = pd.read_parquet(\n",
    "        data_path / \"create_final_relationships.parquet\"\n",
    "    )\n",
    "    final_entities = pd.read_parquet(data_path / \"create_final_entities.parquet\")\n",
    "    final_covariates_path = data_path / \"create_final_covariates.parquet\"\n",
    "    final_covariates = (\n",
    "        pd.read_parquet(final_covariates_path)\n",
    "        if final_covariates_path.exists()\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    vector_store_args = (\n",
    "        config.embeddings.vector_store if config.embeddings.vector_store else {}\n",
    "    )\n",
    "\n",
    "    reporter.info(f\"Vector Store Args: {vector_store_args}\")\n",
    "    vector_store_type = vector_store_args.get(\"type\", VectorStoreType.LanceDB)\n",
    "\n",
    "    entities = read_indexer_entities(final_nodes, final_entities, community_level)\n",
    "    description_embedding_store = __get_embedding_description_store(\n",
    "        entities=entities,\n",
    "        vector_store_type=vector_store_type,\n",
    "        config_args=vector_store_args,\n",
    "    )\n",
    "    covariates = (\n",
    "        read_indexer_covariates(final_covariates)\n",
    "        if final_covariates is not None\n",
    "        else []\n",
    "    )\n",
    "\n",
    "    search_engine = get_local_search_engine(\n",
    "        config,\n",
    "        reports=read_indexer_reports(\n",
    "            final_community_reports, final_nodes, community_level\n",
    "        ),\n",
    "        text_units=read_indexer_text_units(final_text_units),\n",
    "        entities=entities,\n",
    "        relationships=read_indexer_relationships(final_relationships),\n",
    "        covariates={\"claims\": covariates},\n",
    "        description_embedding_store=description_embedding_store,\n",
    "        response_type=response_type,\n",
    "    )\n",
    "\n",
    "    result = search_engine.search(query=query)\n",
    "    return result.response\n",
    "\n",
    "def  _create_default_config(root_dir, config_path: str, api_key, llm_model):\n",
    "    with open(config_path, \"rb\") as file:\n",
    "        import yaml\n",
    "\n",
    "        content = file.read().decode(encoding=\"utf-8\", errors=\"strict\")\n",
    "        content = content.replace(\"${OPENAI_API_KEY}\", api_key)\n",
    "        content = content.replace(\"${LLM_MODEL}\", llm_model)\n",
    "        data = yaml.safe_load(content)\n",
    "        return create_graphrag_config(data, root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating llm client with {'api_key': 'REDACTED,len=56', 'type': \"openai_chat\", 'model': 'gpt-4o-mini', 'max_tokens': 4000, 'temperature': 0.0, 'top_p': 1.0, 'n': 1, 'request_timeout': 180.0, 'api_base': None, 'api_version': None, 'organization': None, 'proxy': None, 'cognitive_services_endpoint': None, 'deployment_name': None, 'model_supports_json': True, 'tokens_per_minute': 0, 'requests_per_minute': 0, 'max_retries': 10, 'max_retry_wait': 10.0, 'sleep_on_rate_limit_recommendation': True, 'concurrent_requests': 25}\n"
     ]
    }
   ],
   "source": [
    "search_engine = run_global_search(\n",
    "    \"./config/graphrag.yaml\",\n",
    "    \"./data/graphrag/gs_q6OLkTkx9NIQ0Wobtqn62tOy/output/default/artifacts\", \n",
    "    \"./data/graphrag/gs_q6OLkTkx9NIQ0Wobtqn62tOy\", \n",
    "    2, \n",
    "    response_type=\"multiple paragraphs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43msearch_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m主人公について教えて\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/learn_graphrag/.venv/lib/python3.10/site-packages/graphrag/query/structured_search/global_search/search.py:164\u001b[0m, in \u001b[0;36mGlobalSearch.search\u001b[0;34m(self, query, conversation_history, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch\u001b[39m(\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    159\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    160\u001b[0m     conversation_history: ConversationHistory \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    162\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GlobalSearchResult:\n\u001b[1;32m    163\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform a global search synchronously.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconversation_history\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/lib/python3.10/asyncio/runners.py:33\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m coroutines\u001b[38;5;241m.\u001b[39miscoroutine(main):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma coroutine was expected, got \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(main))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "result = search_engine.asearch(\n",
    "    \"主人公について教えて\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
