{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024 Microsoft Corporation.\n",
    "# Licensed under the MIT License\n",
    "\n",
    "\"\"\"Main definition.\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import warnings\n",
    "import uvloop \n",
    "from graphrag.config import create_graphrag_config\n",
    "import os\n",
    "from graphrag.index import PipelineConfig, create_pipeline_config\n",
    "import yaml\n",
    "from graphrag.index.run import run_pipeline_with_config\n",
    "\n",
    "# Ignore warnings from numba\n",
    "warnings.filterwarnings(\"ignore\", message=\".*NumbaDeprecationWarning.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"FutureWarning\")\n",
    "\n",
    "api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "llm_model = \"gpt-4o-mini\"\n",
    "\n",
    "def index(\n",
    "    root: str,\n",
    "    config: str | None,\n",
    "):\n",
    "    \"\"\"Run the pipeline with the given config.\"\"\"\n",
    "    run_id = \"test\"\n",
    "    pipeline_config: str | PipelineConfig = _create_default_config(\n",
    "        root, config\n",
    "    )\n",
    "\n",
    "    def _run_workflow_async() -> None:\n",
    "        async def execute():\n",
    "            async for output in run_pipeline_with_config(\n",
    "                pipeline_config,\n",
    "                run_id=run_id,\n",
    "            ):\n",
    "                if output.errors and len(output.errors) > 0:\n",
    "                    return\n",
    "        \n",
    "        uvloop.install()\n",
    "\n",
    "        # This part replaces asyncio.run(execute())\n",
    "        if not asyncio.get_event_loop().is_running():\n",
    "            asyncio.run(execute())\n",
    "        else:\n",
    "            asyncio.create_task(execute())\n",
    "    _run_workflow_async()\n",
    "\n",
    "def _create_default_config(\n",
    "    root_dir: str,\n",
    "    config_path: str ,\n",
    ") -> PipelineConfig:\n",
    "    \"\"\"Overlay default values on an existing config or create a default config if none is provided.\"\"\"\n",
    "    with open(config_path, \"rb\") as file:\n",
    "        file_content = file.read().decode(encoding=\"utf-8\", errors=\"strict\")\n",
    "        replaced_content = file_content.replace(\"${OPENAI_API_KEY}\", api_key).replace(\"${LLM_MODEL}\", llm_model)\n",
    "        data = yaml.safe_load(replaced_content)\n",
    "        parameters = create_graphrag_config(data, root_dir)\n",
    "\n",
    "    result = create_pipeline_config(parameters)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './data'\n",
    "config = './config/graphrag_index.yaml'\n",
    "\n",
    "# index_cli 関数を呼び出す\n",
    "index(\n",
    "    root=root,\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs-N32muS6XFsSnYSgiptk7O37y'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "\"gs_\" + \"\".join(\n",
    "                random.choices(string.ascii_letters + string.digits, k=24)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Copyright (c) 2024 Microsoft Corporation.\n",
    "# Licensed under the MIT License\n",
    "\n",
    "\"\"\"Command line interface for the query module.\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import cast\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from graphrag.config import (\n",
    "    GraphRagConfig,\n",
    "    create_graphrag_config,\n",
    ")\n",
    "from graphrag.index.progress import PrintProgressReporter\n",
    "from graphrag.model.entity import Entity\n",
    "from graphrag.query.input.loaders.dfs import (\n",
    "    store_entity_semantic_embeddings,\n",
    ")\n",
    "from graphrag.vector_stores import VectorStoreFactory, VectorStoreType\n",
    "from graphrag.vector_stores.lancedb import LanceDBVectorStore\n",
    "\n",
    "from graphrag.query.factories import get_global_search_engine, get_local_search_engine\n",
    "from graphrag.query.indexer_adapters import (\n",
    "    read_indexer_covariates,\n",
    "    read_indexer_entities,\n",
    "    read_indexer_relationships,\n",
    "    read_indexer_reports,\n",
    "    read_indexer_text_units,\n",
    ")\n",
    "\n",
    "reporter = PrintProgressReporter(\"\")\n",
    "\n",
    "api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "llm_model = \"gpt-4o-mini\"\n",
    "def __get_embedding_description_store(\n",
    "    entities: list[Entity],\n",
    "    vector_store_type: str = VectorStoreType.LanceDB,\n",
    "    config_args: dict | None = None,\n",
    "):\n",
    "    \"\"\"Get the embedding description store.\"\"\"\n",
    "    if not config_args:\n",
    "        config_args = {}\n",
    "\n",
    "    collection_name = config_args.get(\n",
    "        \"query_collection_name\", \"entity_description_embeddings\"\n",
    "    )\n",
    "    config_args.update({\"collection_name\": collection_name})\n",
    "    description_embedding_store = VectorStoreFactory.get_vector_store(\n",
    "        vector_store_type=vector_store_type, kwargs=config_args\n",
    "    )\n",
    "\n",
    "    description_embedding_store.connect(**config_args)\n",
    "\n",
    "    if config_args.get(\"overwrite\", False):\n",
    "        # this step assumps the embeddings where originally stored in a file rather\n",
    "        # than a vector database\n",
    "\n",
    "        # dump embeddings from the entities list to the description_embedding_store\n",
    "        store_entity_semantic_embeddings(\n",
    "            entities=entities, vectorstore=description_embedding_store\n",
    "        )\n",
    "    else:\n",
    "        # load description embeddings to an in-memory lancedb vectorstore\n",
    "        # to connect to a remote db, specify url and port values.\n",
    "        description_embedding_store = LanceDBVectorStore(\n",
    "            collection_name=collection_name\n",
    "        )\n",
    "        description_embedding_store.connect(\n",
    "            db_uri=config_args.get(\"db_uri\", \"./lancedb\")\n",
    "        )\n",
    "\n",
    "        # load data from an existing table\n",
    "        description_embedding_store.document_collection = (\n",
    "            description_embedding_store.db_connection.open_table(\n",
    "                description_embedding_store.collection_name\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return description_embedding_store\n",
    "\n",
    "\n",
    "def run_global_search(\n",
    "    config_path: str | None,\n",
    "    data_dir: str | None,\n",
    "    root_dir: str | None,\n",
    "    community_level: int,\n",
    "    response_type: str,\n",
    "):\n",
    "    \"\"\"Run a global search with the given query.\"\"\"\n",
    "    config =  _create_default_config(\n",
    "        root_dir, config_path, api_key, llm_model\n",
    "    )\n",
    "    data_path = Path(data_dir)\n",
    "\n",
    "    final_nodes: pd.DataFrame = pd.read_parquet(\n",
    "        data_path / \"create_final_nodes.parquet\"\n",
    "    )\n",
    "    final_entities: pd.DataFrame = pd.read_parquet(\n",
    "        data_path / \"create_final_entities.parquet\"\n",
    "    )\n",
    "    final_community_reports: pd.DataFrame = pd.read_parquet(\n",
    "        data_path / \"create_final_community_reports.parquet\"\n",
    "    )\n",
    "\n",
    "    reports = read_indexer_reports(\n",
    "        final_community_reports, final_nodes, community_level\n",
    "    )\n",
    "    entities = read_indexer_entities(final_nodes, final_entities, community_level)\n",
    "    search_engine = get_global_search_engine(\n",
    "        config,\n",
    "        reports=reports,\n",
    "        entities=entities,\n",
    "        response_type=response_type,\n",
    "    )\n",
    "\n",
    "    return search_engine\n",
    "\n",
    "\n",
    "def run_local_search(\n",
    "    config_path: str | None,\n",
    "    data_dir: str | None,\n",
    "    root_dir: str | None,\n",
    "    community_level: int,\n",
    "    response_type: str,\n",
    "    query: str,\n",
    "):\n",
    "    \"\"\"Run a local search with the given query.\"\"\"\n",
    "    config =  _create_default_config(\n",
    "        root_dir ,config_path, api_key, llm_model\n",
    "    )\n",
    "    data_path = Path(data_dir)\n",
    "\n",
    "    final_nodes = pd.read_parquet(data_path / \"create_final_nodes.parquet\")\n",
    "    final_community_reports = pd.read_parquet(\n",
    "        data_path / \"create_final_community_reports.parquet\"\n",
    "    )\n",
    "    final_text_units = pd.read_parquet(data_path / \"create_final_text_units.parquet\")\n",
    "    final_relationships = pd.read_parquet(\n",
    "        data_path / \"create_final_relationships.parquet\"\n",
    "    )\n",
    "    final_entities = pd.read_parquet(data_path / \"create_final_entities.parquet\")\n",
    "    final_covariates_path = data_path / \"create_final_covariates.parquet\"\n",
    "    final_covariates = (\n",
    "        pd.read_parquet(final_covariates_path)\n",
    "        if final_covariates_path.exists()\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    vector_store_args = (\n",
    "        config.embeddings.vector_store if config.embeddings.vector_store else {}\n",
    "    )\n",
    "\n",
    "    reporter.info(f\"Vector Store Args: {vector_store_args}\")\n",
    "    vector_store_type = vector_store_args.get(\"type\", VectorStoreType.LanceDB)\n",
    "\n",
    "    entities = read_indexer_entities(final_nodes, final_entities, community_level)\n",
    "    description_embedding_store = __get_embedding_description_store(\n",
    "        entities=entities,\n",
    "        vector_store_type=vector_store_type,\n",
    "        config_args=vector_store_args,\n",
    "    )\n",
    "    covariates = (\n",
    "        read_indexer_covariates(final_covariates)\n",
    "        if final_covariates is not None\n",
    "        else []\n",
    "    )\n",
    "\n",
    "    search_engine = get_local_search_engine(\n",
    "        config,\n",
    "        reports=read_indexer_reports(\n",
    "            final_community_reports, final_nodes, community_level\n",
    "        ),\n",
    "        text_units=read_indexer_text_units(final_text_units),\n",
    "        entities=entities,\n",
    "        relationships=read_indexer_relationships(final_relationships),\n",
    "        covariates={\"claims\": covariates},\n",
    "        description_embedding_store=description_embedding_store,\n",
    "        response_type=response_type,\n",
    "    )\n",
    "\n",
    "    result = search_engine.search(query=query)\n",
    "    return result.response\n",
    "\n",
    "def  _create_default_config(root_dir, config_path: str, api_key, llm_model):\n",
    "    with open(config_path, \"rb\") as file:\n",
    "        import yaml\n",
    "\n",
    "        content = file.read().decode(encoding=\"utf-8\", errors=\"strict\")\n",
    "        content = content.replace(\"${OPENAI_API_KEY}\", api_key)\n",
    "        content = content.replace(\"${LLM_MODEL}\", llm_model)\n",
    "        data = yaml.safe_load(content)\n",
    "        return create_graphrag_config(data, root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating llm client with {'api_key': 'REDACTED,len=56', 'type': \"openai_chat\", 'model': 'gpt-4o-mini', 'max_tokens': 4000, 'temperature': 0.0, 'top_p': 1.0, 'n': 1, 'request_timeout': 180.0, 'api_base': None, 'api_version': None, 'organization': None, 'proxy': None, 'cognitive_services_endpoint': None, 'deployment_name': None, 'model_supports_json': True, 'tokens_per_minute': 0, 'requests_per_minute': 0, 'max_retries': 10, 'max_retry_wait': 10.0, 'sleep_on_rate_limit_recommendation': True, 'concurrent_requests': 25}\n"
     ]
    }
   ],
   "source": [
    "search_engine = run_global_search(\n",
    "    \"./config/graphrag.yaml\",\n",
    "    \"./data/graphrag/gs_q6OLkTkx9NIQ0Wobtqn62tOy/output/default/artifacts\", \n",
    "    \"./data/graphrag/gs_q6OLkTkx9NIQ0Wobtqn62tOy\", \n",
    "    2, \n",
    "    response_type=\"multiple paragraphs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 主人公の概要\n",
      "\n",
      "漫画『忘却バッテリー』の主人公は、物語の中心的なキャラクターであり、その経験や挑戦が物語全体を通じて描かれています。彼の旅は、高校野球や個人の成長といった広範なテーマを反映しており、スポーツが若者のアイデンティティやコミュニティの絆を形成する重要性を強調しています。このような背景により、主人公は読者にとって共感を呼ぶ存在となっています [Data: Reports (2, 0)]。\n",
      "\n",
      "## 物語のテーマと成長\n",
      "\n",
      "主人公の成長は、野球が彼の人生に与える影響を通じて明らかになります。物語は、彼が直面するさまざまな挑戦を通じて、自己発見や友情、努力の重要性を描写しています。これにより、読者は主人公の成長過程に感情移入しやすくなります。\n",
      "\n",
      "## 作者の影響\n",
      "\n",
      "『忘却バッテリー』の作者である三川栄子は、主人公のキャラクター形成において重要な役割を果たしています。彼女の演劇におけるバックグラウンドは、主人公の深みや複雑さに影響を与えている可能性があります。彼女は自身の経験を活かし、魅力的な物語を創り出すことで、主人公の芸術的な進化を描いています [Data: Reports (0)]。\n",
      "\n",
      "## 結論\n",
      "\n",
      "このように、『忘却バッテリー』の主人公は、スポーツを通じた成長と自己発見の物語を体現しており、読者にとって非常に魅力的なキャラクターです。彼の経験は、青春時代の葛藤や友情の重要性を強調し、物語全体に深い意味を与えています。\n"
     ]
    }
   ],
   "source": [
    "result = await search_engine.asearch(\n",
    "    \"主人公について教えて\"\n",
    ")\n",
    "\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
